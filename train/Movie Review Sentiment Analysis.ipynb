{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression based classification\n",
    "\n",
    "#### Import data from the IMDB training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:34\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "df=pd.DataFrame()\n",
    "for s in ('test','train'):\n",
    "    for l in ('pos','neg'):\n",
    "        path = './aclImdb/%s/%s' % (s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path,file),'rt', encoding=\"utf8\") as infile:\n",
    "                txt = infile.read()\n",
    "                df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "                pbar.update()\n",
    "df.columns = ['review','sentiment']\n",
    "df.to_csv('./movie_data.csv', index=False) #save for use in future SGD step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess text and convert it into a bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "df=df.reindex(np.random.permutation(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+',' ',text.lower()) + ' '.join(emoticons).replace('-','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "X_train=df.loc[:25,'review'].values\n",
    "y_train=df.loc[:25,'sentiment'].values\n",
    "\n",
    "X_test=df.loc[25000:,'review'].values\n",
    "y_test=df.loc[25000:,'sentiment'].values\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop = stopwords.words('English')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter=PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup SciKit pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "tfidf = TfidfVectorizer(strip_accents=False, preprocessor=None, lowercase=False)\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'vect__ngram_range': [(1,1)],\n",
    "        'vect__stop_words': [stop, None],\n",
    "        'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "        'clf__penalty': ['l1','l2'],\n",
    "        'clf__C': [1.0, 10.0, 100.0]        \n",
    "    },\n",
    "    {\n",
    "        'vect__ngram_range': [(1,1)],\n",
    "        'vect__stop_words': [stop, None],\n",
    "        'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "        'vect__use_idf': [False],\n",
    "        'vect__norm': [None],\n",
    "        'clf__penalty': ['l1','l2'],\n",
    "        'clf__C': [1.0, 10.0, 100.0]        \n",
    "    }\n",
    "    ]\n",
    "best_param_grid = [\n",
    "    {\n",
    "        'vect__ngram_range': [(1,1)],\n",
    "        'vect__stop_words': [None],\n",
    "        'vect__tokenizer': [tokenizer],\n",
    "        'clf__penalty': [l2'],\n",
    "        'clf__C': [10.0]        \n",
    "    }\n",
    "]\n",
    "lr = LogisticRegression(random_state=0)\n",
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', lr)])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, best_param_grid, scoring='accuracy', n_jobs=1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the best estimator and run the classifier on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "\n",
    "print(\"Test Accuracy: %0.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Online Learning_ using Stochastic Gradient Descent\n",
    "\n",
    "The above method is fine for a static one-time training set. However we would like to be able to fine-tune the classifier based on new data that is available, for example, based on feedback provided by end-users or from additional training sets.\n",
    "\n",
    "So for our final solution we use Stochastic Gradient Descent instead. It also happens to be way faster: on my laptop, training with 50,000 reviews took under a minute with SGD but took almost an hour with the best LR classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup tokenizer and stream the reviews in small batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+',' ',text.lower()) + ' '.join(emoticons).replace('-','')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "def stream_docs(path): \n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv)\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label\n",
    "\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [],[]\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error='ignore',#norm='l2',ngram_range=(2,2),\n",
    "                            n_features=2**21, preprocessor=None, tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n",
    "\n",
    "doc_stream = stream_docs(path='./movie_data.csv')\n",
    "\n",
    "import pyprind\n",
    "num_batches = 45\n",
    "pbar = pyprind.ProgBar(num_batches)\n",
    "train_batch_size = 1000\n",
    "test_batch_size = 5000\n",
    "\n",
    "classes = np.array([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(num_batches):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=train_batch_size)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()\n",
    "\n",
    "X_test, y_test = get_minibatch(doc_stream, size=test_batch_size)\n",
    "X_test = vect.transform(X_test)\n",
    "\n",
    "print(\"Test Accuracy: %0.3f\" % clf.score(X_test, y_test))\n",
    "\n",
    "clf = clf.partial_fit(X_test, y_test, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have performed training on servers on large datasets. However we want to run predictions in a web application \n",
    "(where users can interact). So we need to have the trained classifier on the webserver. We achieve this by capturing the \n",
    "latest state of the relevant Python objects using Python \"pickling\" and copy them over to the webserver.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "\n",
    "dest = os.path.join('movieclassifier', 'pickled_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "pickle.dump(stop,\n",
    "    open(os.path.join(dest, 'stopwords.pkl'), 'wb'),\n",
    "    protocol=4)    \n",
    "pickle.dump(clf,\n",
    "    open(os.path.join(dest, 'classifier.pkl'), 'wb'),\n",
    "    protocol=4) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
